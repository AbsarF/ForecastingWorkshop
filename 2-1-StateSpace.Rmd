---
title: "Forecasting: principles and practice"
author: "Rob J Hyndman"
date: "2.1&nbsp; State space models"
fontsize: 14pt
output:
  beamer_presentation:
    theme: Monash
    toc: no
    fig_width: 7
    fig_height: 4.3
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, warning=FALSE, message=FALSE)
library(fpp2)
source("nicefigs.R")
```


# Innovations state space models
##Methods V Models

###Exponential smoothing methods

  * Algorithms that return point forecasts.

\pause

###Innovations state space models

  * Generate same point forecasts but can also generate forecast intervals.
  * A stochastic (or random) data generating process that can generate an entire forecast distribution.
  * Allow for "proper" model selection.

##ETS models

   * Each model has an \textit{observation} equation and \textit{transition} equations, one for each state (level, trend, seasonal), i.e., state space models.
   * Two models for each method: one with additive and one with multiplicative errors, i.e., in total \color{orange}{18 models}.
   * ETS(Error,Trend,Seasonal):
      * Error $=\{$A,M$\}$
      * Trend $=\{$N,A,A\damped$\}$
      * Seasonal $=\{$N,A,M$\}$.


##Exponential smoothing methods
\fontsize{13}{15}\sf

\begin{block}{}
\begin{tabular}{ll|ccc}
& &\multicolumn{3}{c}{\bf Seasonal Component} \\
\multicolumn{2}{c|}{\bf Trend}& N & A & M\\
\multicolumn{2}{c|}{\bf Component}  & ~(None)~    & (Additive)  & (Multiplicative)\\
\cline{3-5} &&&&\\[-0.3cm]
N & (None) & N,N & N,A & N,M\\
&&&&\\[-0.3cm]
A & (Additive) & A,N & A,A & A,M\\
&&&&\\[-0.3cm]
A\damped & (Additive damped) & A\damped,N & A\damped,A & A\damped,M
\end{tabular}
\end{block}

\vspace*{10cm}

##Exponential smoothing methods
\fontsize{13}{15}\sf

\begin{block}{}
\begin{tabular}{ll|ccc}
& &\multicolumn{3}{c}{\bf Seasonal Component} \\
\multicolumn{2}{c|}{\bf Trend}& N & A & M\\
\multicolumn{2}{c|}{\bf Component}  & ~(None)~    & (Additive)  & (Multiplicative)\\
\cline{3-5} &&&&\\[-0.3cm]
N & (None) & N,N & N,A & N,M\\
&&&&\\[-0.3cm]
A & (Additive) & A,N & A,A & A,M\\
&&&&\\[-0.3cm]
A\damped & (Additive damped) & A\damped,N & A\damped,A & A\damped,M
\end{tabular}
\end{block}

\begin{tabular}{l@{}p{2.3cm}@{}c@{}l}
\structure{General n\rlap{otation}}
    &       & ~E T S~  & ~:\hspace*{0.3cm}\textbf{E}xponen\textbf{T}ial \textbf{S}moothing               \\ [-0.2cm]
    & \hfill{$\nearrow$\hspace*{-0.1cm}}        & {$\uparrow$} & {\hspace*{-0.2cm}$\nwarrow$} \\
    & \hfill{\textbf{E}rror\hspace*{0.2cm}} & {\textbf{T}rend}      & {\hspace*{0.2cm}\textbf{S}easonal}
\end{tabular}
\pause\vspace*{-0.4cm}

\structure{Examples:}\newline\footnotesize\vspace*{-0.5cm}

\begin{tabular}{ll}
A,N,N: &Simple exponential smoothing with additive errors\\
A,A,N: &Holt's linear method with additive errors\\

M,A,M: &Multiplicative Holt-Winters' method with multiplicative errors
\end{tabular}

\pause
\color{orange}{\bf There are 18 separate models in the ETS framework}


##A model for SES

\begin{block}{Component form}\vspace*{-0.6cm}
\begin{align*}
\text{Forecast equation}&&\pred{y}{t+h}{t} &= \ell_{t}\\
\text{Smoothing equation}&&\ell_{t} &= \alpha y_{t} + (1 - \alpha)\ell_{t-1}
\end{align*}
\end{block}\pause
Forecast error: $e_t = y_t - \pred{y}{t}{t-1} = y_t - \ell_{t-1}$.\pause
\begin{block}{Error correction form}\vspace*{-0.6cm}
\begin{align*}
y_t &= \ell_{t-1} + e_t\\
\ell_{t}
         &= \ell_{t-1}+\alpha( y_{t}-\ell_{t-1})\\
         &= \ell_{t-1}+\alpha e_{t}
\end{align*}
\end{block}\pause\vspace*{-0.2cm}

Specify probability distribution for $e_t$, we assume $e_t = \varepsilon_t\sim\text{NID}(0,\sigma^2)$.

##ETS(A,N,N)

\begin{block}{}\vspace*{-0.6cm}
\begin{align*}
\text{Measurement equation}&& y_t &= \ell_{t-1} + \varepsilon_t\\
\text{State equation}&& \ell_t&=\ell_{t-1}+\alpha \varepsilon_t
\end{align*}
\end{block}
where $\varepsilon_t\sim\text{NID}(0,\sigma^2)$.

  * "innovations" or "single source of error" because same error process, $\varepsilon_t$.
  * Measurement equation: relationship between observations and states.
  * Transition equation(s): evolution of the state(s) through time.

##ETS(M,N,N)

SES with multiplicative errors.

  * Specify relative errors  $\varepsilon_t=\frac{y_t-\pred{y}{t}{t-1}}{\pred{y}{t}{t-1}}\sim \text{NID}(0,\sigma^2)$
  * Substituting $\pred{y}{t}{t-1}=\ell_{t-1}$ gives:
    * $y_t = \ell_{t-1}+\ell_{t-1}\varepsilon_t$
    * $e_t = y_t - \pred{y}{t}{t-1} = \ell_{t-1}\varepsilon_t$

 \pause
\begin{block}{}\vspace*{-0.6cm}
\begin{align*}
\text{Measurement equation}&& y_t &= \ell_{t-1}(1 + \varepsilon_t)\\
\text{State equation}&& \ell_t&=\ell_{t-1}(1+\alpha \varepsilon_t)
\end{align*}
\end{block}
\pause

  * Models with additive and multiplicative errors with the same parameters generate the same point forecasts but different prediction intervals.

##ETS(A,A,N)

Holt's linear method with additive errors.

  * Assume $\varepsilon_t=y_t-\ell_{t-1}-b_{t-1} \sim \text{NID}(0,\sigma^2)$.
  * Substituting into the error correction equations for Holt's linear method\vspace*{-0.8cm}
  \begin{align*}
      y_t&=\ell_{t-1}+b_{t-1}+\varepsilon_t\\
      \ell_t&=\ell_{t-1}+b_{t-1}+\alpha \varepsilon_t\\
      b_t&=b_{t-1}+\alpha\beta^* \varepsilon_t
  \end{align*}
  * For simplicity, set $\beta=\alpha \beta^*$.

##ETS(M,A,N)

Holt's linear method with multiplicative errors.

  * Assume $\varepsilon_t=\frac{y_t-(\ell_{t-1}+b_{t-1})}{(\ell_{t-1}+b_{t-1})}$
  * Following a similar approach as above, the innovations state space model underlying Holt's linear method with multiplicative errors is specified as\vspace*{-0.8cm}
  \begin{align*}
      y_t&=(\ell_{t-1}+b_{t-1})(1+\varepsilon_t)\\
      \ell_t&=(\ell_{t-1}+b_{t-1})(1+\alpha \varepsilon_t)\\
      b_t&=b_{t-1}+\beta(\ell_{t-1}+b_{t-1}) \varepsilon_t
  \end{align*}
  where again  $\beta=\alpha \beta^*$ and $\varepsilon_t \sim \text{NID}(0,\sigma^2)$.

##ETS(A,A,A)

Holt-Winters additive method with additive errors.

\begin{block}{}\vspace*{-0.6cm}
\begin{align*}
\text{Forecast equation} && \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} + s_{t-m+h_m^+}\\
\text{Observation equation}&& y_t&=\ell_{t-1}+b_{t-1}+s_{t-m} + \varepsilon_t\\
\text{State equations}&& \ell_t&=\ell_{t-1}+b_{t-1}+\alpha \varepsilon_t\\
&&        b_t&=b_{t-1}+\beta \varepsilon_t \\
&&s_t &= s_{t-m} + \gamma\varepsilon_t
\end{align*}
\end{block}

* Forecast errors: $\varepsilon_{t} = y_t - \hat{y}_{t|t-1}$
* $h_m^+ = \lfloor (h-1) \mod m\rfloor + 1$.


##Additive error models

\placefig{0}{1.2}{width=12.8cm,trim=0 120 0 0,clip=true}{fig_7_ets_add.pdf}


##Multiplicative error models

\placefig{0}{1.2}{width=12.8cm,trim=0 120 0 0,clip=true}{fig_7_ets_multi.pdf}


##Estimating ETS models

  * Smoothing parameters $\alpha$, $\beta$, $\gamma$ and $\phi$, and the initial states $\ell_0$, $b_0$, $s_0,s_{-1},\dots,s_{-m+1}$ are estimated by maximising the "likelihood" = the probability of the data arising from the specified model.
  * For models with additive errors equivalent to minimising SSE.
  * For models with multiplicative errors, \textbf{not} equivalent to minimising SSE.
  * We will estimate models with the \Verb|ets()| function in the forecast package.

##Innovations state space models

Let $\bm{x}_t = (\ell_t, b_t, s_t, s_{t-1}, \dots, s_{t-m+1})$ and
$\varepsilon_t\stackrel{\mbox{\scriptsize iid}}{\sim
\mbox{N}(0,\sigma^2)}$.
\begin{block}{}
\begin{tabular}{lcl}
$y_t$ &=& $\underbrace{h(\bm{x}_{t-1})} +
\underbrace{k(\bm{x}_{t-1})\varepsilon_t}$\\
&& \hspace*{0.5cm}$\mu_t$ \hspace*{1.45cm} $e_t$ \\[0.2cm]
$\bm{x}_t$ &=& $f(\bm{x}_{t-1}) +
g(\bm{x}_{t-1})\varepsilon_t$\\
\end{tabular}
\end{block}


Additive errors
: \mbox{}\newline
  $k(x)=1$.\qquad $y_t = \mu_{t} + \varepsilon_t$.

Multiplicative errors
: \mbox{}\newline
  $k(\bm{x}_{t-1}) = \mu_{t}$.\qquad $y_t = \mu_{t}(1 + \varepsilon_t)$.\newline
  $\varepsilon_t = (y_t - \mu_t)/\mu_t$ is relative error.

##Innovations state space models

\structure{Estimation}\vspace*{0.5cm}

\begin{block}{}
\begin{align*}
L^*(\bm\theta,\bm{x}_0) &= n\log\!\bigg(\sum_{t=1}^n \varepsilon^2_t/k^2(\bm{x}_{t-1})\!\bigg) + 2\sum_{t=1}^n \log|k(\bm{x}_{t-1})|\\
&= -2\log(\text{Likelihood}) + \mbox{constant}
\end{align*}
\end{block}

* Estimate parameters $\bm\theta = (\alpha,\beta,\gamma,\phi)$ and
initial states $\bm{x}_0 = (\ell_0,b_0,s_0,s_{-1},\dots,s_{-m+1})$ by
minimizing $L^*$.

##Parameter restrictions
\fontsize{13}{15}\sf

\structure{\textit{Usual} region}

  * Traditional restrictions in the methods $0< \alpha,\beta^*,\gamma^*,\phi<1$\newline (equations interpreted as weighted averages).
  * In models we set $\beta=\alpha\beta^*$ and $\gamma=(1-\alpha)\gamma^*$.
  * Therefore $0< \alpha <1$, &nbsp;&nbsp; $0 < \beta < \alpha$ &nbsp;&nbsp; and $0< \gamma < 1-\alpha$.
  * $0.8<\phi<0.98$ --- to prevent numerical difficulties.
 \pause

\structure{\textit{Admissible} region}

  * To prevent observations in the distant past having a continuing effect on current forecasts.
  * Usually (but not always) less restrictive than the \textit{usual} region.
  * For example for ETS(A,N,N): \newline \textit{usual} $0< \alpha <1$ --- \textit{admissible} is $0< \alpha <2$.

##Model selection
\begin{block}{Akaike's Information Criterion}
\[
\text{AIC} = -2\log(\text{L}) + 2k
\]
\end{block}\vspace*{-0.2cm}
where $L$ is the likelihood and $k$ is the number of parameters initial states estimated in the model.\pause

\begin{block}{Corrected AIC}
\[
\text{AIC}_{\text{c}} = \text{AIC} + \frac{2(k+1)(k+2)}{T-k}
\]
\end{block}
which is the AIC corrected (for small sample bias).
\pause
\begin{block}{Bayesian Information Criterion}
\[
\text{BIC} = \text{AIC} + k(\log(T)-2).
\]
\end{block}


##Automatic forecasting

**From Hyndman et al.\ (IJF, 2002):**

* Apply each model that is appropriate to the data.
Optimize parameters and initial values using MLE (or some other
criterion).
* Select best method using AICc:
* Produce forecasts using best method.
* Obtain Forecast intervals using underlying state space model.

Method performed very well in M3 competition.


##Some unstable models

* Some of the combinations of (Error, Trend, Seasonal) can lead to numerical difficulties; see equations with division by a state.
* These are: ETS(A,N,M), ETS(A,A,M), ETS(A,A\damped,M).
* Models with multiplicative errors are useful for strictly positive data, but are not numerically stable with data containing zeros or negative values. In that case only the six fully additive models will be applied.


##Exponential smoothing models
\fontsize{12}{14}\sf

\begin{block}{}
\begin{tabular}{ll|ccc}
  \multicolumn{2}{l}{\alert{\bf Additive Error}} &        \multicolumn{3}{c}{\bf Seasonal Component}         \\
          \multicolumn{2}{c|}{\bf Trend}         &         N         &         A         &         M         \\
        \multicolumn{2}{c|}{\bf Component}       &     ~(None)~      &    (Additive)     & (Multiplicative)  \\ \cline{3-5}
           &                                     &                   &                   &  \\[-0.3cm]
  N        & (None)                              &       A,N,N       &       A,N,A       &    \st{A,N,M}     \\
           &                                     &                   &                   &  \\[-0.3cm]
  A        & (Additive)                          &       A,A,N       &       A,A,A       &    \st{A,A,M}     \\
           &                                     &                   &                   &  \\[-0.3cm]
  A\damped & (Additive damped)                   &   A,A\damped,N    &   A,A\damped,A    & \st{A,A\damped,M}
\end{tabular}
\end{block}

\begin{block}{}
\begin{tabular}{ll|ccc}
  \multicolumn{2}{l}{\alert{\bf Multiplicative Error}} &     \multicolumn{3}{c}{\bf Seasonal Component}      \\
             \multicolumn{2}{c|}{\bf Trend}            &      N       &         A         &        M         \\
           \multicolumn{2}{c|}{\bf Component}          &   ~(None)~   &    (Additive)     & (Multiplicative) \\ \cline{3-5}
           &                                           &              &                   &  \\[-0.3cm]
  N        & (None)                                    &    M,N,N     &       M,N,A       &      M,N,M       \\
           &                                           &              &                   &  \\[-0.3cm]
  A        & (Additive)                                &    M,A,N     &       M,A,A       &      M,A,M       \\
           &                                           &              &                   &  \\[-0.3cm]
  A\damped & (Additive damped)                         & M,A\damped,N &   M,A\damped,A    &   M,A\damped,M
\end{tabular}
\end{block}



##Forecasting with ETS models

\structure{Point forecasts:} iterate the equations for $t=T+1,T+2,\dots,T+h$ and set all $\varepsilon_t=0$ for $t>T$.\newline\pause

* Not the same as $\text{E}(y_{t+h} | \bm{x}_t)$ unless trend and seasonality are both additive.
* Point forecasts for ETS(A,x,y) are identical to ETS(M,x,y) if the parameters are the same.

##Forecasting with ETS models

\structure{Prediction intervals:} cannot be generated using the methods.

  * The prediction intervals will differ between models with additive and multiplicative methods.
  * Exact formulae for some models.
  * More general to simulate future sample paths, conditional on the last estimate of the states, and to obtain prediction intervals from the percentiles of these simulated future paths.
  * Options are available in R using the `forecast` function in the forecast package.

# ETS in R


##Example: drug sales
\fontsize{8}{9}\sf

```{r, echo=TRUE}
ets(h02)
```


##Example: drug sales
\fontsize{8}{9}\sf

```{r, echo=TRUE}
ets(h02, model="AAA", damped=FALSE)
```

##The `ets()` function

* Automatically chooses a model by default using the AIC, AICc or BIC.
* Can handle any combination of trend, seasonality and damping
* Produces prediction intervals for every model
* Ensures the parameters are admissible (equivalent to invertible)
* Produces an object of class "ets".

## `ets` objects

* **Methods:** `coef()`, `autoplot()`, `plot()`, `summary()`, `residuals()`, `fitted()`, `simulate()` and `forecast()`
* `autoplot()` and `plot()` functions show time plots of the original time series along with the extracted components (level, growth and seasonal).

##Example: drug sales
```{r, echo=TRUE, fig.height=4}
h02 %>% ets %>% autoplot
```

##Example: drug sales

```{r, echo=TRUE, fig.height=4}
h02 %>% ets %>% forecast %>% autoplot
```


##Example: drug sales
\fontsize{9}{12}\sf

```{r, echo=TRUE}
h02 %>% ets %>% accuracy

h02 %>% ets(model="AAA", damped=FALSE) %>% accuracy
```

##The `ets()` function
\fontsize{9}{10}\sf

`ets()` function also allows refitting model to new data set.

```{r, echo=TRUE}
train <- window(h02, end=c(2004,12))
test <- window(h02, start=2005)
fit1 <- ets(train)
fit2 <- ets(test, model = fit1)
accuracy(fit2)
accuracy(forecast(fit1,10), test)
```

##The `ets()` function in R


```r
ets(y, model = "ZZZ", damped = NULL,
  additive.only = FALSE,
  lambda = NULL, biasadj = FALSE,
  lower = c(rep(1e-04, 3), 0.8),
  upper = c(rep(0.9999, 3), 0.98),
  opt.crit = c("lik", "amse", "mse", "sigma", "mae"),
  nmse = 3,
  bounds = c("both", "usual", "admissible"),
  ic = c("aicc", "aic", "bic"),
  restrict = TRUE,
  allow.multiplicative.trend = FALSE, ...)
```

##The `ets()` function in R
\fontsize{13}{15}\sf

* `y` \newline The time series to be forecast.
* `model` \newline use the ETS classification and notation: "N" for none, "A" for additive, "M" for multiplicative, or "Z" for automatic selection. Default `ZZZ` all components are selected using the information criterion.
* `damped`
  * If `damped=TRUE`, then a damped trend will be used (either A\damped\ or M\damped).
  * `damped=FALSE`, then a non-damped trend will used.
  * If `damped=NULL` (the default), then either a damped or a non-damped trend will be selected according to the information criterion chosen.


##The `ets()` function in R
\fontsize{13}{14.5}\sf\vspace*{-0.2cm}

  * `additive.only`\newline
      Only models with additive components will be considered if \Verb"additive.only=TRUE". Otherwise all models will be considered.
  * `lambda`\newline
      Box-Cox transformation parameter. It will be ignored if `lambda=NULL` (the default value). Otherwise, the time series will be transformed before the model is estimated. When `lambda` is not `NULL`, `additive.only` is set to `TRUE`.
  * `biadadj`\newline
      Uses bias-adjustment when undoing Box-Cox transformation for fitted values.

##The `ets()` function in R
\fontsize{13}{15}\sf

* `lower,upper` bounds for the parameter estimates of $\alpha$, $\beta^*$, $\gamma^*$ and $\phi$.
* `opt.crit=lik` (default) optimisation criterion used for estimation.
* `bounds` Constraints on the parameters.

    * \textit{usual} region -- `"bounds=usual"`;
    * \textit{admissible} region -- `"bounds=admissible"`;
    * `"bounds=both"` (the default) requires the parameters to satisfy both sets of constraints.

* `ic=aicc` (the default) information criterion to be used in selecting models.
* `restrict=TRUE` (the default) models that cause numerical difficulties are not considered in model selection.
* `allow.multiplicative.trend` allows models with a multiplicative trend.

##The `forecast()` function in R
\fontsize{13}{14.5}\sf

```r
  forecast(object,
    h=ifelse(object$m>1, 2*object$m, 10),
    level=c(80,95), fan=FALSE,
    simulate=FALSE, bootstrap=FALSE,
    npaths=5000, PI=TRUE,
    lambda=object$lambda, biasadj=FALSE,...)
```

* `object`: the object returned by the \Verb|ets()| function.
* `h`: the number of periods to be forecast.
* `level`: the confidence level for the prediction intervals.
* `fan`: if `fan=TRUE`, suitable for fan plots.
* `simulate`: If `TRUE`, prediction intervals generated via simulation rather than analytic formulae. Even if `FALSE` simulation will be used if no algebraic formulae exist.

##The `forecast()` function in R
\fontsize{13}{15}\sf

* `bootstrap`: If `bootstrap=TRUE` and \Verb"simulate=TRUE", then simulated prediction intervals use re-sampled errors rather than normally distributed errors.
* `npaths`: The number of sample paths used in computing simulated prediction intervals.
* `PI`: If `PI=TRUE`, then prediction intervals are produced; otherwise only point forecasts are calculated. If `PI=FALSE`, then `level`, `fan`, `simulate`, `bootstrap` and `npaths` are all ignored.
* `lambda`: The Box-Cox transformation parameter. Ignored if `lambda=NULL`. Otherwise, forecasts are back-transformed via inverse Box-Cox transformation.
* `biasadj`: Apply bias adjustment after Box-Cox?


#Lab session 8
##

\fontsize{48}{60}\sf\centering
\textbf{Lab Session 8}



