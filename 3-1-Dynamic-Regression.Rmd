---
title: "Forecasting: principles and practice"
author: "Rob J Hyndman"
date: "3.1&nbsp; Dynamic regression"
fontsize: 14pt
output:
  beamer_presentation:
    fig_width: 7
    fig_height: 3.5
    highlight: tango
    theme: metropolis
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, warning=FALSE, message=FALSE)
library(fpp2)
```


# Regression with ARIMA errors

## Regression with ARIMA errors

\begin{block}{Regression models}\vspace*{-0.2cm}
\[
y_t = \beta_0 + \beta_1 x_{1,t} + \dots + \beta_k x_{k,t} + e_t,
\]\end{block}

  * $y_t$ modeled as function of $k$ explanatory variables
$x_{1,t},\dots,x_{k,t}$.
  * In regression, we assume that $e_t$ was  WN.
  * Now we want to allow $e_t$ to be autocorrelated.
\vspace*{0.3cm}
\pause
\begin{alertblock}{Example: ARIMA(1,1,1) errors}\vspace*{-0.7cm}
\begin{align*}
y_t &= \beta_0 + \beta_1 x_{1,t} + \dots + \beta_k x_{k,t} + n_t,\\
& (1-\phi_1B)(1-B)n_t = (1+\theta_1B)e_t,
\end{align*}
\end{alertblock}
\rightline{where $e_t$ is white noise .}


## Residuals and errors

\begin{alertblock}{Example: $n_t$ = ARIMA(1,1,1)}\vspace*{-0.7cm}
\begin{align*}
y_t &= \beta_0 + \beta_1 x_{1,t} + \dots + \beta_k x_{k,t} + n_t,\\
& (1-\phi_1B)(1-B)n_t = (1+\theta_1B)e_t,
\end{align*}\end{alertblock}\pause

  * Be careful in distinguishing $n_t$ from $e_t$.
  * Only the errors $n_t$ are assumed to be white noise.
  * In ordinary regression, $n_t$ is assumed to be white noise and so $n_t = e_t$.



## Estimation

If we minimize $\sum n_t^2$ (by using ordinary regression):

  1. Estimated coefficients $\hat{\beta}_0,\dots,\hat{\beta}_k$ are no longer optimal as some information ignored;
  2. Statistical tests associated with the model (e.g., t-tests on the coefficients) are incorrect.
  3. $p$-values for coefficients usually too small (``spurious regression'').
  4. AIC of fitted models misleading.

\pause


 * Minimizing $\sum e_t^2$ avoids these problems.
 * Maximizing likelihood is similar to minimizing $\sum e_t^2$.


## Stationarity

\begin{block}{Model with ARIMA(1,1,1) errors}\vspace*{-0.7cm}
\begin{align*}
y_t &= \beta_0 + \beta_1 x_{1,t} + \dots + \beta_k x_{k,t} + n_t,\\
& (1-\phi_1B)(1-B)n_t = (1+\theta_1B)e_t,
\end{align*}
\end{block}\pause

\begin{block}{Equivalent to model with ARIMA(1,0,1) errors}\vspace*{-0.7cm}
\begin{align*}
y'_t &= \beta_1 x'_{1,t} + \dots + \beta_k x'_{k,t} + n'_t,\\
& (1-\phi_1B)n'_t = (1+\theta_1B)e_t,
\end{align*}
\end{block}
where $y'_t=y_t-y_{t-1}$, $x'_{t,i}=x_{t,i}-x_{t-1,i}$ and  $n'_t=n_t-n_{t-1}$.


## Regression with ARIMA errors

Any regression with an ARIMA
error can be rewritten as a regression with an ARMA error by differencing all
variables with the same differencing operator as in the ARIMA model.\pause

\begin{block}{Original data}\vspace*{-0.7cm}
\begin{align*}
y_t &= \beta_0 + \beta_1 x_{1,t} + \dots + \beta_k x_{k,t} + n_t\\
\mbox{where}\quad
& \phi(B)(1-B)^dn_t = \theta(B)e_t
\end{align*}\end{block}\pause\vspace*{-0.1cm}
\begin{block}{After differencing all variables}\vspace*{-0.7cm}
\begin{align*}
y'_t &= \beta_1 x'_{1,t} + \dots + \beta_k x'_{k,t} + n'_t.\\
\mbox{where}\quad
& \phi(B)n_t = \theta(B)e_t \\
\text{and}\quad & y_t' = (1-B)^dy_t
\end{align*}
\end{block}




## Model selection

  * Fit regression model with automatically selected ARIMA errors.
  * Check that $e_t$ series looks like white noise.
  * Note that estimation is done on the differenced series to ensure consistent estimators.

### Selecting predictors
\begin{itemize}
\item AICc can be calculated for final model.
\item Repeat procedure for all subsets of predictors to be considered, and select model with lowest AICc value.
\end{itemize}

##\large US personal consumption \& income

```{r usconsump}
autoplot(uschange, facets=TRUE) +
  xlab("Year") + ylab("") +
  ggtitle("Quarterly changes in US consumption and personal income")
```

##\large US personal consumption \& income


```{r}
ggplot(aes(x=Income,y=Consumption), data=as.data.frame(uschange)) +
  geom_point() +
  ggtitle("Quarterly changes in US consumption and personal income")
```


##\large US Personal Consumption and income

  * No need for transformations or further differencing.
  *  Increase in income does not necessarily translate into instant increase in consumption (e.g., after the loss of a job, it may take a few months for expenses to be reduced to allow for the new circumstances). We will ignore this for now.


##\large US personal consumption \& income
\fontsize{10}{14}\sf

```{r usconsump2, echo=TRUE, fig.height=3}
(fit <- auto.arima(uschange[,"Consumption"],
    xreg=uschange[,"Income"]))
```

##\large US personal consumption \& income

```{r , echo=TRUE, fig.height=3.7}
ggtsdisplay(residuals(fit, type='regression'),
  main="ARIMA errors")
```

##\large US personal consumption \& income

```{r , echo=TRUE, fig.height=3.7}
ggtsdisplay(residuals(fit),
  main="ARIMA residuals")
```

##\large US Personal Consumption and Income
\fontsize{12}{14}\sf

A Ljung-Box test shows the residuals are uncorrelated.

```{r, echo=TRUE}
checkresiduals(fit, plot=FALSE)
```

##\large US Personal Consumption and Income
\fontsize{11}{13}\sf

```{r usconsump3, echo=TRUE, fig.height=3.}
fcast <- forecast(fit,
  xreg=rep(mean(uschange[,"Income"]),8), h=8)
autoplot(fcast) + xlab("Year") +
  ylab("Percentage change") +
  ggtitle("Forecasts from regression with ARIMA(1,0,2) errors")
```


## Forecasting

  * To forecast a regression model with ARIMA errors, we need to forecast the
regression part of the model and the ARIMA part of the model and combine the
results.

    * Some explanatory variable are known into the future (e.g., time, dummies).
    * Separate forecasting models may be needed for other explanatory variables.

# Stochastic and deterministic trends

##\large Stochastic \& deterministic trends

\structure{Deterministic trend}
\[ y_t = \beta_0 + \beta_1 t + n_t \]
where $n_t$ is ARMA process.\pause

\structure{Stochastic trend}
\[ y_t = \beta_0 + \beta_1 t + n_t \]
where $n_t$ is ARIMA process with $d\ge1$.\pause

Difference both sides until $n_t$ is stationary:
\[ y'_t = \beta_1 + n'_t \]
where $n'_t$ is ARMA process.



## International visitors

```{r}
autoplot(austa) + xlab("Year") +
  ylab("millions of people") +
  ggtitle("Total annual international visitors to Australia")
```

## International visitors
\fontsize{11}{12}\sf

\structure{Deterministic trend}

```{r, echo=TRUE}
(fit1 <- auto.arima(austa, d=0, xreg=1:length(austa)))
```

\pause\vspace*{-0.6cm}

\begin{align*}
y_t &= 0.4173 + 0.1715 t + n_t \\
n_t &= 1.0371 n_{t-1} - 0.3379 n_{t-2} + e_t\\
e_t &\sim \text{NID}(0,0.02854).
\end{align*}

## International visitors
\fontsize{11}{12}\sf

\structure{Stochastic trend}

```{r, echo=TRUE}
(fit2 <- auto.arima(austa,d=1))
```

\pause\vspace*{-0.6cm}

\begin{align*}
y_t-y_{t-1} &= 0.1537 + e_t \\
y_t &= y_0 + 0.1537 t + n_t \\
n_t &= n_{t-1} + e_{t}\\
e_t &\sim \text{NID}(0,0.03241).
\end{align*}


## International visitors

```{r, fig.height=2.2}
autoplot(forecast(fit1, xreg=length(austa) + 1:10)) +
  xlab("Year") + ylab("") +
  ggtitle("Forecasts from linear trend with AR(2) error")
```

```{r, fig.height=2.2}
autoplot(forecast(fit2)) +
  xlab("Year") + ylab("") +
  ggtitle("Forecasts from ARIMA(0,1,0) with drift")
```

## Forecasting with trend

  * Point forecasts are almost identical, but prediction intervals differ.
  * Stochastic trends have much wider prediction intervals because the errors are non-stationary.
  * Be careful of forecasting with deterministic trends too far ahead.


# Periodic seasonality

## Fourier terms for seasonality

Periodic seasonality can be handled using pairs of Fourier terms:
$$
s_{k}(t) = \sin\left(\frac{2\pi k t}{m}\right)\qquad c_{k}(t) = \cos\left(\frac{2\pi k t}{m}\right)
$$
$$
y_t = \sum_{k=1}^K \left[\alpha_k s_k(t) + \beta_k c_k(t)\right] + n_t$$

  * $n_t$ is non-seasonal ARIMA process.
  * Every periodic function can be approximated by sums of sin and cos terms for large enough $K$.
  * Choose $K$ by minimizing AICc.




## US Accidental Deaths

```{r, echo=TRUE}
fit <- auto.arima(USAccDeaths,
         xreg=fourier(USAccDeaths, 5),
         seasonal=FALSE)

fc <- forecast(fit,
         xreg=fourier(USAccDeaths, 5, 24))
```

## US Accidental Deaths

```{r, echo=TRUE, fig.height=4}
autoplot(fc)
```

# Lab session 14
##

\fontsize{48}{60}\sf\centering
\textbf{Lab Session 14}


# Dynamic regression models

## Dynamic regression models

\structure{Sometimes a change in $x_t$ does not affect $y_t$ instantaneously}\pause
\begin{block}{}
\begin{itemize}
  \item  $y_t=$ sales, $x_t=$ advertising.
  \item  $y_t=$ stream flow, $x_t=$ rainfall.
  \item  $y_t=$ size of herd, $x_t=$ breeding stock.
\end{itemize}
\end{block}
\pause

  * These are dynamic systems with input ($x_t$) and output $(y_t)$.
  * $x_t$ is often a leading indicator.
  * There can be multiple predictors.



## Lagged explanatory variables

The model include present and past values of predictor: $x_t,x_{t-1},x_{t-2},\dots.$
\begin{block}{}
\centerline{$
y_t = a + \nu_0x_t + \nu_1x_{t-1} + \dots + \nu_kx_{t-k} + n_t$}
\end{block}
where $n_t$ is an ARIMA process.\pause

\structure{Rewrite model as }\vspace*{-0.9cm}
\begin{align*}
y_{t} & = a+ (\nu_{0} + \nu_{1} B + \nu_{2} B^{2} + \dots + \nu_{k} B^{k}) x_{t} +n_t \\
      & = a+ \nu(B) x_{t} +n_t.
\end{align*}\pause\vspace*{-0.3cm}

  * $\nu(B)$ is called a \textit{transfer function} since it describes how
change in $x_t$ is transferred to $y_t$.
  * $x$ can influence $y$, but $y$ is not allowed to influence $x$.




##\large Example: Insurance quotes and TV adverts


```{r tvadvert}
autoplot(insurance, facets=TRUE) +
  xlab("Year") + ylab("") +
  ggtitle("Insurance advertising and quotations")
```

##\large Example: Insurance quotes and TV adverts
\fontsize{10}{11}\sf

```{r, echo=TRUE, cache=TRUE}
Advert <- cbind(insurance[,2], c(NA,insurance[1:39,2]))
colnames(Advert) <- paste("AdLag",0:1,sep="")
(fit <- auto.arima(insurance[,1], xreg=Advert, d=0))
```
\pause\vspace*{-0.9cm}

\begin{align*}
y_t &= 2.04 + 1.26x_t + 0.16x_{t-1} + n_t \\
n_t &= 1.41n_{t-1} -0.93 n_{t-2} + 0.36n_{t-3} + e_t
\end{align*}


##\large Example: Insurance quotes and TV adverts
\fontsize{11}{13}\sf

```{r, echo=TRUE, fig.height=3.3}
fc <- forecast(fit, h=20,
  xreg=cbind(c(Advert[40,1],rep(10,19)), rep(10,20)))
autoplot(fc)
```


##\large Example: Insurance quotes and TV adverts
\fontsize{11}{13}\sf

```{r, echo=TRUE, fig.height=3.3}
fc <- forecast(fit, h=20,
  xreg=cbind(c(Advert[40,1],rep(8,19)), rep(8,20)))
autoplot(fc)
```

##\large Example: Insurance quotes and TV adverts
\fontsize{11}{13}\sf

```{r, echo=TRUE, fig.height=3.3}
fc <- forecast(fit, h=20,
  xreg=cbind(c(Advert[40,1],rep(6,19)), rep(6,20)))
autoplot(fc)
```

## Dynamic regression models

\begin{block}{}\centerline
{$y_t = a + \nu(B) x_t + n_t$}
\end{block}
where $n_t$ is an ARMA process. So $$\phi(B)n_t = \theta(B) e_t\qquad\text{or}\qquad
n_t = \frac{\theta(B)}{\phi(B)}e_t = \psi(B) e_t.$$\pause\vspace*{-0.5cm}
\begin{block}{}\centerline
{$y_t = a + \nu(B) x_t +  \psi(B)e_t$}
\end{block}\pause\vspace*{-0.3cm}

  * ARMA models are rational approximations to general transfer functions of $e_t$.
  * We can also replace $\nu(B)$ by a rational approximation.
  * There is no R package for forecasting using a general transfer function approach.


