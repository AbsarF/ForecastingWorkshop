---
title: "Forecasting: principles and practice"
author: "Rob J Hyndman"
date: "1.3&nbsp; Forecast evaluation"
fontsize: 14pt
output:
  beamer_presentation:
    fig_width: 7
    fig_height: 3.5
    highlight: tango
    theme: metropolis
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, warning=FALSE, message=FALSE)
library(fpp2)
```


# Benchmark methods

## Some simple forecasting methods
\small\vspace*{-0.5cm}

```{r, fig.height=4.6, echo=FALSE}
beer2 <- window(ausbeer, start=1992)
autoplot(beer2) +
  xlab("Year") + ylab("megalitres") +
    ggtitle("Australian quarterly beer production")
```

\begin{textblock}{7}(0.2,8.7)
\begin{alertblock}{}
\small{How would you forecast these data?}
\end{alertblock}
\end{textblock}

## Some simple forecasting methods
\small\vspace*{-0.5cm}

```{r, fig.height=4.6, echo=FALSE}
autoplot(window(pigs/1e3, start=1990)) +
  xlab("Year") + ylab("thousands") +
  ggtitle("Number of pigs slaughtered in Victoria")
```

\begin{textblock}{7}(0.2,8.7)
\begin{alertblock}{}
\small{How would you forecast these data?}
\end{alertblock}
\end{textblock}

## Some simple forecasting methods
\small\vspace*{-0.5cm}

```{r, fig.height=4.6, echo=FALSE}
autoplot(dj) + xlab("Day") +
  ggtitle("Dow-Jones index") + ylab("")
```

\begin{textblock}{7}(0.2,8.7)
\begin{alertblock}{}
\small{How would you forecast these data?}
\end{alertblock}
\end{textblock}

## Some simple forecasting methods

\fontsize{13}{15}\sf

### Average method

  * Forecast of all future values is equal to mean of historical data $\{y_1,\dots,y_T\}$.
  * Forecasts: $\hat{y}_{T+h|T} = \bar{y} = (y_1+\dots+y_T)/T$

\pause

### Naïve method

  * Forecasts equal to last observed value.
  * Forecasts: $\hat{y}_{T+h|T} =y_T$.
  * Consequence of efficient market hypothesis.

\pause

### Seasonal naïve method

  * Forecasts equal to last value from same season.
  * Forecasts: $\hat{y}_{T+h|T} =y_{T+h-km}$ where $m=$ seasonal period and $k=\lfloor (h-1)/m\rfloor$+1.

## Some simple forecasting methods

### Drift method

 * Forecasts equal to last value plus average change.
 * Forecasts:\vspace*{-1cm}

 \begin{align*}
 \hat{y}_{T+h|T} & =  y_{T} + \frac{h}{T-1}\sum_{t=2}^T (y_t-y_{t-1})\\
                 & = y_T + \frac{h}{T-1}(y_T -y_1).
 \end{align*}\vspace*{-0.5cm}

   * Equivalent to extrapolating a line drawn between first and last observations.

## Some simple forecasting methods

```{r beerf, warning=FALSE, message=FALSE, echo=FALSE, fig.height=4.6}
beer2 <- window(ausbeer,start=1992,end=c(2007,4))
# Plot some forecasts
autoplot(beer2) +
  forecast::autolayer(meanf(beer2, h=11), PI=FALSE, series="Mean") +
  forecast::autolayer(naive(beer2, h=11), PI=FALSE, series="Naïve") +
  forecast::autolayer(snaive(beer2, h=11), PI=FALSE, series="Seasonal naïve") +
  ggtitle("Forecasts for quarterly beer production") +
  xlab("Year") + ylab("Megalitres") +
  guides(colour=guide_legend(title="Forecast"))
```

## Some simple forecasting methods

```{r djf,  message=FALSE, warning=FALSE, echo=FALSE, fig.height=4.6}
# Set training data to first 250 days
dj2 <- window(dj,end=250)
# Plot some forecasts
autoplot(dj2) +
  forecast::autolayer(meanf(dj2, h=42), PI=FALSE, series="Mean") +
  forecast::autolayer(rwf(dj2, h=42), PI=FALSE, series="Naïve") +
  forecast::autolayer(rwf(dj2, drift=TRUE, h=42), PI=FALSE, series="Drift") +
  ggtitle("Dow Jones Index (daily ending 15 Jul 94)") +
  xlab("Day") + ylab("") +
  guides(colour=guide_legend(title="Forecast"))
```

## Some simple forecasting methods

  * Mean: `meanf(y, h=20)`
  * Naïve:  `naive(y, h=20)`
  * Seasonal naïve: `snaive(y, h=20)`
  * Drift: `rwf(y, drift=TRUE, h=20)`

# Forecasting residuals

## Fitted values

 - $\hat{y}_{t|t-1}$ is the forecast of $y_t$ based on observations $y_1,\dots,y_t$.
 - We call these "fitted values".
 - Sometimes drop the subscript: $\hat{y}_t \equiv \hat{y}_{t|t-1}$.
 - Often not true forecasts since parameters are estimated on all data.

###For example:

 - $\hat{y}_{t} = \bar{y}$ for average method.
 - $\hat{y}_{t} = y_{t-1} + (y_{T}-y_1)/(T-1)$ for drift method.

## Forecasting residuals

\begin{block}{}
\textbf{Residuals in forecasting:} difference between observed value and its fitted value: $e_t = y_t-\hat{y}_{t|t-1}$.
\end{block}
\pause\fontsize{13}{15}\sf

\structure{Assumptions}

  1. $\{e_t\}$ uncorrelated. If they aren't, then information left in  residuals that should be used in computing forecasts.
  2. $\{e_t\}$ have mean zero. If they don't, then forecasts are biased.

\pause

\structure{Useful properties} (for prediction intervals)

  3. $\{e_t\}$ have constant variance.
  4. $\{e_t\}$ are normally distributed.

## Example: Dow-Jones index

```{r, echo=FALSE, fig.height=4.6}
dj2 <- window(dj, end=250)
autoplot(dj2) + xlab("Day") + ylab("") +
  ggtitle("Dow Jones Index (daily ending 15 Jul 94)")
```

## Example: Dow-Jones index

\structure{Na\"{\i}ve forecast:}

\[\hat{y}_{t|t-1}= y_{t-1}\]\pause
\[e_t = y_t-y_{t-1}\]\pause

\begin{alertblock}{}
Note: $e_t$ are one-step-forecast residuals
\end{alertblock}

## Example: Dow-Jones index

```{r}
res <- residuals(naive(dj2))
autoplot(res) + xlab("Day") + ylab("") +
  ggtitle("Residuals from naive method")
```

## Example: Dow-Jones index

```{r}
qplot(res, bins=nclass.FD(na.omit(res))) +
  ggtitle("Histogram of residuals")
```

## Example: Dow-Jones index

```{r}
ggAcf(res) + ggtitle("ACF of residuals")
```

## ACF of residuals

  * We assume that the residuals are white noise (uncorrelated, mean zero, constant variance). If they aren't, then there is information left in  the residuals that should be used in computing forecasts.

  * So a standard residual diagnostic is to check the ACF of the residuals of a forecasting method.

  * We \emph{expect} these to look like white noise.

## Portmanteau tests

Consider a \textit{whole set} of $r_{k}$  values, and develop a test to see whether the set is significantly different from a zero set.\pause

\begin{block}{Box-Pierce test\phantom{g}}
\centerline{$\displaystyle
Q = T \sum_{k=1}^h r_k^2$}
where $h$  is max lag being considered and $T$ is number of observations.
\end{block}

  * My preferences: $h=10$ for non-seasonal data, $h=2m$ for seasonal data.
  * If each $r_k$ close to zero, $Q$ will be **small**.
  * If some $r_k$ values large (positive or negative), $Q$ will be **large**.

## Portmanteau tests

Consider a \textit{whole set} of $r_{k}$  values, and develop a test to see whether the set is significantly different from a zero set.

\begin{block}{Ljung-Box test}
\centerline{$\displaystyle
 Q^* = T(T+2) \sum_{k=1}^h (T-k)^{-1}r_k^2$}
where $h$  is max lag being considered and $T$ is number of observations.
\end{block}

  * My preferences: $h=10$ for non-seasonal data, $h=2m$ for seasonal data.
  * Better performance, especially in small samples.

\vspace*{10cm}

## Portmanteau tests

  * If data are WN, $Q^*$ has $\chi^2$ distribution with  $(h - K)$ degrees of freedom where $K=$ no.\ parameters in model.
  * When applied to raw data, set $K=0$.
  * For the Dow-Jones example,

\fontsize{12}{13}\sf

```{r, echo=TRUE}
# lag=h and fitdf=K
Box.test(res, lag=10, fitdf=0)
```

## Portmanteau tests

  * If data are WN, $Q^*$ has $\chi^2$ distribution with  $(h - K)$ degrees of freedom where $K=$ no.\ parameters in model.
  * When applied to raw data, set $K=0$.
  * For the Dow-Jones example,

\fontsize{12}{13}\sf

```{r, echo=TRUE}
# lag=h and fitdf=K
Box.test(res, lag=10, fitdf=0, type="Lj")
```

## `checkresiduals` function

```{r, echo=TRUE, fig.height=4}
checkresiduals(naive(dj2))
```

## `checkresiduals` function

```{r, echo=FALSE}
object <- naive(dj2)
main <- paste("Residuals from", object$method)
res <- residuals(object)
# Do Ljung-Box test
      LBtest <- Box.test(zoo::na.approx(res), fitdf=0, lag=10, type="Ljung")
      LBtest$method <- "Ljung-Box test"
      LBtest$data.name <- main
      names(LBtest$statistic) <- "Q*"
      print(LBtest)
      cat(paste("Model df: ",0,".   Total lags used: ",10,"\n\n",sep=""))
```

#Lab session 3
##

\fontsize{48}{60}\sf\centering
\textbf{Lab Session 3}

# Evaluating forecast accuracy

## Measures of forecast accuracy

Let $y_t$ denote the $t$th observation and $\pred{y}{t}{t-1}$ denote its forecast based on all previous data, where $t=1,\dots,T$. Then the following measures are useful.\vspace*{-0.7cm}

\begin{align*}
\text{MAE} &= T^{-1}\sum_{t=1}^T |y_t - \pred{y}{t}{t-1}| \\[-0.2cm]
\text{MSE} &= T^{-1}\sum_{t=1}^T (y_t - \pred{y}{t}{t-1})^2 \quad
\text{RMSE} &= \sqrt{T^{-1}\sum_{t=1}^T (y_t - \pred{y}{t}{t-1})^2} \\[-0.1cm]
\text{MAPE} &= 100T^{-1}\sum_{t=1}^T |y_t - \pred{y}{t}{t-1}|/|y_t|
\end{align*}\pause\vspace*{-0.2cm}

  * MAE, MSE, RMSE are all scale dependent.
  * MAPE is scale independent but is only sensible if $y_t\gg 0$ for all $t$, and $y$ has a natural zero.

## Measures of forecast accuracy

\begin{block}{Mean Absolute Scaled Error}
$$
\text{MASE} = T^{-1}\sum_{t=1}^T |y_t - \pred{y}{t}{t-1}|/Q
$$
where $Q$ is a stable measure of the scale of the time series $\{y_t\}$.
\end{block}
Proposed by Hyndman and Koehler (IJF, 2006).

For non-seasonal time series,
$$
  Q = (T-1)^{-1}\sum_{t=2}^T |y_t-y_{t-1}|
$$
works well. Then MASE is equivalent to MAE relative to a naïve method.

\vspace*{10cm}

## Measures of forecast accuracy

\begin{block}{Mean Absolute Scaled Error}
$$
\text{MASE} = T^{-1}\sum_{t=1}^T |y_t - \pred{y}{t}{t-1}|/Q
$$
where $Q$ is a stable measure of the scale of the time series $\{y_t\}$.
\end{block}
Proposed by Hyndman and Koehler (IJF, 2006).

For seasonal time series,
$$
  Q = (T-m)^{-1}\sum_{t=m+1}^T |y_t-y_{t-m}|
$$
works well. Then MASE is equivalent to MAE relative to a seasonal naïve method.

\vspace*{10cm}

## Measures of forecast accuracy

```{r beeraccuracy}
beer2 <- window(ausbeer,start=1992,end=c(2005,4))
beerfc1 <- meanf(beer2,h=10)
beerfc2 <- rwf(beer2,h=10)
beerfc3 <- snaive(beer2,h=10)
tmp <- cbind(Data=window(ausbeer, start=1992),
             Mean=beerfc1$mean,
             Naive=beerfc2$mean,
             SeasonalNaive=beerfc3$mean)
autoplot(tmp) + xlab("Year") + ylab("Megalitres") +
  ggtitle("Forecasts for quarterly beer production") +
  scale_color_manual(values=c('#000000','#1b9e77','#d95f02','#7570b3'),
                     breaks=c("Mean","Naive","SeasonalNaive"),
                     name="Forecast Method")
```

## Measures of forecast accuracy

```{r echo=FALSE}
beer3 <- window(ausbeer, start=2006)
tab <- matrix(NA,ncol=4,nrow=3)
tab[1,] <- accuracy(beerfc1, ausbeer)[2,c(2,3,5,6)]
tab[2,] <- accuracy(beerfc2, ausbeer)[2,c(2,3,5,6)]
tab[3,] <- accuracy(beerfc3, ausbeer)[2,c(2,3,5,6)]
colnames(tab) <- c("RMSE","MAE","MAPE","MASE")
rownames(tab) <- c("Mean method", "Naïve method", "Seasonal naïve method")
knitr::kable(tab, digits=2)
```

## Training and test sets

\begin{tabular}{|c|c|}
\multicolumn{2}{c}{\structure{Available data}}\\
\hline
\hspace*{8.7cm} & \\
Training set & Test set \\
(e.g., 80\%) & (e.g., 20\%) \\
&\\
\hline
\end{tabular}

  * The test set must not be used for \emph{any} aspect of model development or calculation of forecasts.

  * Forecast accuracy is based only on the test set.

## Training and test sets
\fontsize{13}{15}\sf

\fontsize{13}{16}\sf

```r
beer2 <- window(ausbeer, start=1992, end=c(2005,4))
fc <- snaive(beer2, h=10)
accuracy(fc, ausbeer)
```

\pause

\structure{In-sample accuracy} (one-step forecasts)

```
accuracy(fc)
```

## Beware of over-fitting

  * A model which fits the data well does not necessarily forecast well.

  * A perfect fit can always be obtained by using a model with enough parameters. (Compare $R^2$)

  *  Over-fitting a model to data is as bad as failing to identify the systematic pattern in the data.

  * Problems can be overcome by measuring true \emph{out-of-sample} forecast accuracy.  That is, total data divided into "training" set and "test" set.  Training set used to estimate parameters. Forecasts are made for test set.

  * Accuracy measures computed for errors in test set only.

## Poll: true or false?

  1. Good forecast methods should have normally distributed residuals.
  2. A model with small residuals will give good forecasts.
  3. The best measure of forecast accuracy is MAPE.
  4. If your model doesn't forecast well, you should make it more complicated.
  5. Always choose the model with the best forecast accuracy as measured on the test set.

#Lab session 4
##

\fontsize{48}{60}\sf\centering
\textbf{Lab Session 4}

