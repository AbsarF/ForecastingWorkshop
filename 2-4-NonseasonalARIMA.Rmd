---
title: "Forecasting: principles and practice"
author: "Rob J Hyndman"
date: "2.4&nbsp; Non-seasonal ARIMA models"
fontsize: 14pt
output:
  beamer_presentation:
    fig_width: 7
    fig_height: 3.5
    highlight: tango
    theme: metropolis
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, warning=FALSE, message=FALSE)
library(fpp2)
library(gridExtra)
```

# Autoregressive models

## Autoregressive models

\begin{block}{Autoregressive (AR) models:}
$$
  y_{t}  =  c  +  \phi_{1}y_{t - 1}  +  \phi_{2}y_{t - 2}  +  \cdots  +  \phi_{p}y_{t - p}  + e_{t},
$$
where $e_t$ is white noise.  This is a multiple regression with \textbf{lagged values} of $y_t$ as predictors.
\end{block}

```{r arp, echo=FALSE, fig.height=3}
set.seed(1)
p1 <- autoplot(10 + arima.sim(list(ar = -0.8), n = 100)) +
  ylab("") + ggtitle("AR(1)")
p2 <- autoplot(20 + arima.sim(list(ar = c(1.3, -0.7)), n = 100)) +
  ylab("") + ggtitle("AR(2)")
grid.arrange(p1,p2,nrow=1)
```

## AR(1) model

\begin{block}{}
\centerline{$y_{t}    =   2 -0.8 y_{t - 1}  +  e_{t}$}
\end{block}

$e_t\sim N(0,1)$,\quad $T=100$.

```{r, echo=FALSE, fig.height=2.6, fig.width=2.6}
p1
```

## AR(1) model

\begin{block}{}
\centerline{$y_{t}    =   c + \phi_1 y_{t - 1}  +  e_{t}$}
\end{block}


* When $\phi_1=0$, $y_t$ is **equivalent to WN**
* When $\phi_1=1$ and $c=0$, $y_t$ is **equivalent to a RW**
* When $\phi_1=1$ and $c\ne0$, $y_t$ is **equivalent to a RW with drift**
* When $\phi_1<0$, $y_t$ tends to **oscillate between positive and negative values**.

## AR(2) model

\begin{block}{}
\centerline{$y_t = 8 + 1.3y_{t-1} - 0.7 y_{t-2} + e_t$}
\end{block}

$e_t\sim N(0,1)$, \qquad $T=100$.

```{r, fig.height=2.6, fig.width=2.6}
p2
```


## Stationarity conditions


We normally restrict autoregressive models to stationary data, and then some constraints on the values of the parameters are required.

\begin{block}{General condition for stationarity}
Complex roots of $1-\phi_1 z - \phi_2 z^2 - \dots - \phi_pz^p$ lie outside the unit circle on the complex plane.
\end{block}\pause

* For $p=1$:  $-1<\phi_1<1$.
* For $p=2$:\newline $-1<\phi_2<1\qquad \phi_2+\phi_1 < 1 \qquad \phi_2 -\phi_1 < 1$.
* More complicated conditions hold for $p\ge3$.
* Estimation software takes care of this.

# Moving average models

## Moving Average (MA) models

\begin{block}{Moving Average (MA) models:}
$$
  y_{t}  =  c +  e_t + \theta_{1}e_{t - 1}  +  \theta_{2}e_{t - 2}  +  \cdots  + \theta_{q}e_{t - q},
$$
where $e_t$ is white noise.
This is a multiple regression with \textbf{past \emph{errors}}
as predictors. \emph{Don't confuse this with moving average smoothing!}
\end{block}

```{r maq, fig.height=2.8}
set.seed(2)
p1 <- autoplot(20 + arima.sim(list(ma = 0.8), n = 100)) +
  ylab("") + ggtitle("MA(1)")
p2 <- autoplot(arima.sim(list(ma = c(-1, +0.8)), n = 100)) +
  ylab("") + ggtitle("MA(2)")
grid.arrange(p1,p2,nrow=1)
```

## MA(1) model

\begin{block}{}
\centerline{$y_t = 20 + e_t + 0.8 e_{t-1}$}
\end{block}

$e_t\sim N(0,1)$,\quad $T=100$.

```{r, fig.height=2.6, fig.width=2.6}
p1
```

## MA(2) model

\begin{block}{}
\centerline{$y_t = e_t -e_{t-1} + 0.8 e_{t-2}$}
\end{block}

$e_t\sim N(0,1)$,\quad $T=100$.

```{r, fig.height=2.6, fig.width=2.6}
p2
```

## Invertibility

* Any MA($q$) process can be written as an AR($\infty$) process if we impose some constraints on the MA parameters.
* Then the MA model is called "invertible".
* Invertible models have some mathematical properties that make them easier to use in practice.
* Invertibility of an ARIMA model is equivalent to forecastability of an ETS model.

## Invertibility

\begin{block}{General condition for invertibility}
Complex roots of $1+\theta_1 z + \theta_2 z^2 + \dots + \theta_qz^q$ lie outside the unit circle on the complex plane.
\end{block}\pause

* For $q=1$:  $-1<\theta_1<1$.
* For $q=2$:\newline $-1<\theta_2<1\qquad \theta_2+\theta_1 >-1 \qquad \theta_1 -\theta_2 < 1$.
* More complicated conditions hold for {$q\ge3$.}
* Estimation software takes care of this.


# Non-seasonal ARIMA models

## ARIMA models

\begin{block}{Autoregressive Moving Average models:}\vspace*{-0.7cm}
\begin{align*}
y_{t}  &=  c  +  \phi_{1}y_{t - 1}  +  \cdots  +  \phi_{p}y_{t - p} \\
& \hspace*{2.4cm}\text{} + \theta_{1}e_{t - 1} +  \cdots  + \theta_{q}e_{t - q} +  e_{t}.
\end{align*}
\end{block}\pause

* Predictors include both **lagged values of $y_t$ and lagged errors.**
* Conditions on coefficients ensure stationarity.
* Conditions on coefficients ensure invertibility.
\pause

### Autoregressive Integrated Moving Average models
* Combine ARMA model with **differencing**.
* $(1-B)^d y_t$ follows an ARMA  model.

## ARIMA models

\structure{Autoregressive Integrated Moving Average models}
\begin{block}{ARIMA($p, d, q$) model}
\begin{tabular}{rl}
AR:& $p =$  order of the autoregressive part\\
I: & $d =$  degree of first differencing involved\\
MA:& $q =$  order of the moving average part.
\end{tabular}
\end{block}

* White noise model:  ARIMA(0,0,0)
* Random walk:  ARIMA(0,1,0) with no constant
* Random walk with drift:  ARIMA(0,1,0) with const.
* AR($p$): ARIMA($p$,0,0)
* MA($q$): ARIMA(0,0,$q$)

## Backshift notation for ARIMA

* ARMA model:\newline
\hspace*{-1.4cm}\parbox{13cm}{\small\begin{align*}
y_{t}  &=  c + \phi_{1}By_{t} + \cdots + \phi_pB^py_{t}
           +  e_{t}  +  \theta_{1}Be_{t} + \cdots + \theta_qB^qe_{t} \\
\text{or}\quad & (1-\phi_1B - \cdots - \phi_p B^p) y_t = c + (1 + \theta_1 B + \cdots + \theta_q B^q)e_t
\end{align*}}
* ARIMA(1,1,1) model:
$$
\begin{array}{c c c c}
(1 - \phi_{1} B) & (1  -  B) y_{t} &= &c + (1  + \theta_{1} B) e_{t}\\
{\uparrow}  & {\uparrow}    &   &{\uparrow}\\
{\text{AR(1)}} & {\text{First}}   &     &{\text{MA(1)}}\\
& {\hbox to 0cm{\hss\text{difference}\hss}}\\
\end{array}
$$\pause
Written out:
$$y_t =   c + y_{t-1} + \phi_1 y_{t-1}- \phi_1 y_{t-2} + \theta_1e_{t-1} + e_t $$


## US personal consumption

```{r}
autoplot(uschange[,"Consumption"]) +
  xlab("Year") +
  ylab("Quarterly percentage change") +
  ggtitle("US consumption")
```

## US personal consumption
\fontsize{11}{13}\sf

```{r, echo=TRUE}
(fit <- auto.arima(uschange[,"Consumption"],
    seasonal=FALSE))
```

\pause\vfill

### ARIMA(0,0,3) or MA(3) model:
\centerline{$y_t = 0.756 + e_t + 0.254 e_{t-1} + 0.226 e_{t-2} + 0.269 e_{t-3},$}
where $e_t$ is white noise with standard deviation
$`r round(sqrt(fit$sigma2),2)` = \sqrt{`r round(fit$sigma2,4)`}$.

## US personal consumption

```{r, echo=TRUE, fig.height=4}
fit %>% forecast(h=10) %>% autoplot(include=80)
```


## Understanding ARIMA models

* If $c=0$ and $d=0$, the long-term forecasts will go to zero.
* If $c=0$ and $d=1$, the long-term forecasts will go to a non-zero constant.
* If $c=0$ and $d=2$, the long-term forecasts will follow a straight line.

* If $c\ne0$ and $d=0$, the long-term forecasts will go to the mean of the data.
* If $c\ne0$ and $d=1$, the long-term forecasts will follow a straight line.
* If $c\ne0$ and $d=2$, the long-term forecasts will follow a quadratic trend.




## Understanding ARIMA models

### Forecast variance and $d$
  * The higher the value of $d$, the more rapidly the prediction intervals increase in size.
  * For $d=0$, the long-term forecast standard deviation will go to the standard deviation of the historical data.

### Cyclic behaviour
  * For cyclic forecasts,  $p>2$ and some restrictions on coefficients are required.
  * If $p=2$, we need $\phi_1^2+4\phi_2<0$. Then average cycle of length
\[
  (2\pi)/\left[\text{arc cos}(-\phi_1(1-\phi_2)/(4\phi_2))\right].
\]


# Partial autocorrelations
## Partial autocorrelations

\structure{Partial autocorrelations} measure relationship\newline
between $y_{t}$  and  $y_{t - k}$, when
the effects of other time lags --- $1,
2, 3, \dots, k - 1$ --- are removed.\pause
\begin{block}{}\vspace*{-0.6cm}
\begin{align*}
\alpha_k&= \text{$k$th partial autocorrelation coefficient}\\
&= \text{equal to the estimate of $b_k$ in regression:}\\
& \hspace*{0.8cm} y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_k y_{t-k}.
\end{align*}
\end{block}\pause

* Varying number of terms on RHS gives $\alpha_k$ for different values of $k$.
* There are more efficient ways of calculating $\alpha_k$.
* $\alpha_1=\rho_1$
* same critical values of $\pm 1.96/\sqrt{T}$ as for ACF.


## Example: US consumption

```{r}
autoplot(uschange[,"Consumption"]) +
  xlab("Year") +
  ylab("Quarterly percentage change") +
  ggtitle("US consumption")
```

## Example: US consumption


```{r usconsumptionacf}
p1 <- ggAcf(uschange[,"Consumption"],main="")
p2 <- ggPacf(uschange[,"Consumption"],main="")
grid.arrange(p1,p2,nrow=1)
```

## ACF and PACF interpretation

**ARIMA($p$,$d$,0)** model if ACF and PACF plots of differenced data show:

* the ACF is exponentially decaying or sinusoidal;
* there is a significant spike at lag $p$ in PACF, but none  beyond lag $p$.
\pause

**ARIMA(0,$d$,$q$)** model if ACF and PACF plots of differenced data show:

* the PACF is exponentially decaying or sinusoidal;
* there is a significant spike at lag $q$ in ACF, but none beyond lag $q$.

## Example: Mink trapping

```{r}
autoplot(mink) +
  xlab("Year") +
  ylab("Minks trapped (thousands)") +
  ggtitle("Annual number of minks trapped")
```

## Example: Mink trapping

```{r}
p1 <- ggAcf(mink,main="")
p2 <- ggPacf(mink,main="")
grid.arrange(p1,p2,nrow=1)
```

# Estimation and order selection

## Maximum likelihood estimation

Having identified the model order, we need to estimate the
parameters $c$, $\phi_1,\dots,\phi_p$,
$\theta_1,\dots,\theta_q$.\pause


* MLE is very similar to least squares estimation obtained by minimizing
$$\sum_{t-1}^T e_t^2.$$
* The `Arima()` command allows CLS or MLE estimation.
* Non-linear optimization must be used in either case.
* Different software will give different estimates.



## Information criteria

\structure{Akaike's Information Criterion (AIC):}
\centerline{$\text{AIC} = -2 \log(L) + 2(p+q+k+1),$}
where $L$ is the likelihood of the data,\newline
$k=1$ if $c\ne0$ and $k=0$ if $c=0$.\pause

\structure{Corrected AIC:}
\[
\text{AICc} = \text{AIC} + \frac{2(p+q+k+1)(p+q+k+2)}{T-p-q-k-2}.
\]\pause

\structure{Bayesian Information Criterion:}
\centerline{$\text{BIC} = \text{AIC} + \log(T)(p+q+k-1).$}
\pause\vspace*{-0.6cm}
\begin{block}{}Good models are obtained by minimizing either the AIC, \text{AICc}\ or BIC\@. Our preference is to use the \text{AICc}.\end{block}


# ARIMA modelling in R

## How does auto.arima() work?

\begin{block}{A non-seasonal ARIMA process}
\[
\phi(B)(1-B)^dy_{t} = c + \theta(B)\varepsilon_t
\]
Need to select appropriate orders: \alert{$p,q, d$}
\end{block}

\structure{Hyndman and Khandakar (JSS, 2008) algorithm:}

  * Select no.\ differences \alert{$d$} and \alert{$D$} via unit root tests.
  * Select \alert{$p,q$} by minimising AICc.
  * Use stepwise search to traverse model space.

## How does auto.arima() work?
\fontsize{12}{14}\sf

\begin{block}{}
\centerline{$\text{AICc} = -2 \log(L) + 2(p+q+k+1)\left[1 +
\frac{(p+q+k+2)}{T-p-q-k-2}\right].$}
where $L$ is the maximised likelihood fitted to the \textit{differenced} data,
$k=1$ if $c\neq 0$ and $k=0$ otherwise.
\end{block}\pause

Step1:
:  Select current model (with smallest AICc) from:\newline
ARIMA$(2,d,2)$\newline
ARIMA$(0,d,0)$\newline
ARIMA$(1,d,0)$\newline
ARIMA$(0,d,1)$
\pause\vspace*{-0.1cm}

Step 2:
:  Consider variations of current model:

    * vary one of $p,q,$ from current model by $\pm1$;
    * $p,q$ both vary from current model by $\pm1$;
    * Include/exclude $c$ from current model.

  Model with lowest AICc becomes current model.

\structure{Repeat Step 2 until no lower AICc can be found.}



## Choosing your own model

```{r, echo=TRUE, fig.height=4}
ggtsdisplay(internet)
```

## Choosing your own model
\fontsize{10}{12}\sf

```{r, echo=TRUE}
tseries::adf.test(internet)
tseries::kpss.test(internet)
```

## Choosing your own model
\fontsize{9}{12}\sf

```{r, echo=TRUE}
tseries::kpss.test(diff(internet))
```

## Choosing your own model

```{r, echo=TRUE, fig.height=4}
internet %>% diff %>% ggtsdisplay
```

## Choosing your own model
\fontsize{13}{15}\sf

```{r, echo=TRUE, fig.height=4}
(fit <- Arima(internet,order=c(3,1,0)))
```

## Choosing your own model
\fontsize{13}{15}\sf

```{r, echo=TRUE, fig.height=4}
(fit2 <- auto.arima(internet))
```

## Choosing your own model

```{r, echo=TRUE, fig.height=4}
checkresiduals(fit, plot=TRUE)
```

## Choosing your own model
\fontsize{13}{15}\sf

```{r, echo=TRUE, fig.height=4}
checkresiduals(fit, plot=FALSE)
```

## Choosing your own model

```{r, echo=TRUE, fig.height=4}
fit %>% forecast %>% autoplot
```



## Modelling procedure with `Arima`
\fontsize{13}{15.5}\sf
\vspace*{-0.2cm}

1. Plot the data. Identify any unusual observations.
2. If necessary, transform the data (using a Box-Cox transformation) to stabilize the variance.
3. If the data are non-stationary: take first differences of the data until the data are stationary.
4. Examine the ACF/PACF: Is an AR($p$) or MA($q$) model appropriate?
5. Try your chosen model(s), and  use the \text{AICc} to search for a better model.
6. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.
7. Once the residuals look like white noise, calculate forecasts.

## Modelling procedure with `auto.arima`
\fontsize{13}{15.5}\sf
\vspace*{-0.4cm}

1. Plot the data. Identify any unusual observations.
2. If necessary, transform the data (using a Box-Cox transformation) to stabilize the variance.

\vfill\vfill\vfill\vfill\vfill\vfill\vfill\vfill\vfill\vfill\vfill

3. Use `auto.arima` to select a model.

\vfill\vfill\vfill\vfill\vfill\vfill\vfill\vfill\vfill\vfill\vfill

6. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.
7. Once the residuals look like white noise, calculate forecasts.

\vspace*{-0.4cm}

## Modelling procedure

\centerline{\includegraphics[height=8.cm]{Figure-8-10}}


##\normalsize Seasonally adjusted electrical equipment
\fontsize{13}{15}\sf

```{r ee1, fig.height=3.3, echo=TRUE}
eeadj <- seasadj(stl(elecequip, s.window="periodic"))
autoplot(eeadj) + xlab("Year") +
  ylab("Seasonally adjusted new orders index")
```


##\normalsize Seasonally adjusted electrical equipment

1. Time plot shows sudden changes, particularly big drop in 2008/2009 due to global economic environment. Otherwise nothing unusual and no need for  data adjustments.
2. No evidence of changing variance, so no Box-Cox transformation.
3. Data are clearly non-stationary, so we take first differences.


## Seasonally adjusted electrical equipment

```{r ee2, echo=TRUE, fig.height=4}
eeadj %>% diff %>% ggtsdisplay
```

##\normalsize Seasonally adjusted electrical equipment

4. PACF is suggestive of AR(3). So initial candidate model is ARIMA(3,1,0). No other obvious candidates.
5. Fit ARIMA(3,1,0) model along with variations: ARIMA(4,1,0), ARIMA(2,1,0), ARIMA(3,1,1), etc. ARIMA(3,1,1) has smallest \text{AICc} value.



##\normalsize Seasonally adjusted electrical equipment
\fontsize{10}{11}\sf

```{r, echo=TRUE}
fit <- Arima(eeadj, order=c(3,1,1))
summary(fit)
```


##\normalsize Seasonally adjusted electrical equipment

6. ACF plot of residuals from ARIMA(3,1,1) model look like white noise.


```{r, echo=TRUE, fig.height=2.5}
ggAcf(residuals(fit))
```

##\normalsize Seasonally adjusted electrical equipment
\fontsize{12}{14}\sf

```{r, echo=TRUE}
checkresiduals(fit, plot=FALSE)
```

##\normalsize Seasonally adjusted electrical equipment

```{r, echo=TRUE}
fit %>% forecast %>% autoplot
```

# Forecasting

## Point forecasts

1. Rearrange ARIMA equation so $y_t$ is on LHS.
2. Rewrite equation by replacing $t$ by $T+h$.
3. On RHS, replace future observations by their forecasts, future errors by zero, and past errors by corresponding residuals.

Start with $h=1$. Repeat for $h=2,3,\dots$.


## Prediction intervals

\begin{block}{95\% Prediction interval}
$$\hat{y}_{T+h|T} \pm 1.96\sqrt{v_{T+h|T}}$$
where $v_{T+h|T}$ is estimated forecast variance.
\end{block}\pause

* $v_{T+1|T}=\hat{\sigma}^2$ for all ARIMA models regardless of parameters and orders.
* Multi-step prediction intervals for ARIMA(0,0,$q$):
\centerline{$\displaystyle y_t = e_t + \sum_{i=1}^q \theta_i e_{t-i}.$}
\centerline{$\displaystyle
v_{T|T+h} = \hat{\sigma}^2 \left[ 1 + \sum_{i=1}^{h-1} \theta_i^2\right], \qquad\text{for~} h=2,3,\dots.$}


## Prediction intervals

\begin{block}{95\% Prediction interval}
$$\hat{y}_{T+h|T} \pm 1.96\sqrt{v_{T+h|T}}$$
where $v_{T+h|T}$ is estimated forecast variance.
\end{block}

* Multi-step prediction intervals for ARIMA(0,0,$q$):
\centerline{$\displaystyle y_t = e_t + \sum_{i=1}^q \theta_i e_{t-i}.$}
\centerline{$\displaystyle
v_{T|T+h} = \hat{\sigma}^2 \left[ 1 + \sum_{i=1}^{h-1} \theta_i^2\right], \qquad\text{for~} h=2,3,\dots.$}

\pause

* AR(1): Rewrite as MA($\infty$) and use above result.
* Other models beyond scope of this workshop.


## Prediction intervals

* Prediction intervals **increase in size with forecast horizon**.
* Prediction intervals can be difficult to calculate by hand
* Calculations assume residuals are **uncorrelated** and **normally distributed**.
* Prediction intervals tend to be too narrow.
    * the uncertainty in the parameter estimates has not been accounted for.
    * the ARIMA model assumes historical patterns will not change during the forecast period.
    * the ARIMA model assumes uncorrelated future errors



# Lab session 11
##

\fontsize{48}{60}\sf\centering
\textbf{Lab Session 11}


